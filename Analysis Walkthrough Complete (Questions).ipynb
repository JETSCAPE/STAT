{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\__init\\__.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Project initialization and common objects. \"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    format='[%(levelname)s][%(module)s] %(message)s',\n",
    "    level=os.getenv('LOGLEVEL', 'info').upper()\n",
    ")\n",
    "\n",
    "workdir = Path(os.getenv('WORKDIR', '.'))\n",
    "\n",
    "cachedir = workdir / 'cache'\n",
    "cachedir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#: Sets the collision systems for the entire project,\n",
    "#: where each system is a string of the form\n",
    "#: ``'<projectile 1><projectile 2><beam energy in GeV>'``,\n",
    "#: such as ``'PbPb2760'``, ``'AuAu200'``, ``'pPb5020'``.\n",
    "#: Even if the project uses only a single system,\n",
    "#: this should still be a list of one system string.\n",
    "systems = ['PbPb5020']\n",
    "\n",
    "\n",
    "#: Design attributes \n",
    "#: keys is a list of strings describing the inputs\n",
    "#: labels is a list of LaTeX labels\n",
    "#: ranges is list of tuples of (min,max) for each design input\n",
    "keys = ['lambda_jet','alpha_s'] #labels in words\n",
    "labels = [r'\\Lambda_{jet}',r'\\alpha_s}'] #labels in LaTeX\n",
    "ranges = [(0.01,0.3),(0.05,0.35)]\n",
    "\n",
    "\n",
    "#Design array to use\n",
    "#Should be a numpy array\n",
    "#If you want to generate a Latin Hypercube with a specific range, set this to None\n",
    "design_array = pickle.load((cachedir / 'lhs/design_s.p').open('rb'))\n",
    "\n",
    "\n",
    "#Dictionary of the model output\n",
    "#Form MUST be data_list_loaded[system][observable][subobservable][{'Y','x'}]\n",
    "###Note - 'Y' is an (n x p) numpy array of the output\n",
    "###       'x' is a (1 x p) numpy array of numeric index of columns of Y (if exists). p_T in example below\n",
    "data_list_loaded = joblib.load(filename = 'cache/model/main/full_data_dict.p')\n",
    "\n",
    "\n",
    "#Dictionary of the experimental data\n",
    "#Form MUST be exp_data_list[system][observable][subobservable][{'y':,'x':,'yerr':{'stat':,'sys'}}]\n",
    "###Note - 'y' is a (1 x p) numpy array of experimental data \n",
    "###       'x' is a (1 x p) numpy array of numeric index of columns of Y (if exists). p_T in example below\n",
    "###       'yerr' is a dictionary with keys 'stat' and 'sys'\n",
    "###       'stat' is a (1 x p) array of statistical errors\n",
    "###       'sys' is a (1 x p) array of systematic errors\n",
    "exp_data_list = joblib.load(filename = 'cache/hepdata/data_list_exp.p')\n",
    "\n",
    "\n",
    "##Experimental covariance matrix\n",
    "##Set exp_cov = None to have the script estimate the covariance matrix\n",
    "exp_cov = joblib.load(filename = 'cache/hepdata/cov_exp_pbpb5020_30_50.p')\n",
    "#exp_cov = None\n",
    "\n",
    "#: Observables to emulate as a list of 2-tuples\n",
    "#: ``(obs, [list of subobs])``.\n",
    "observables = [('R_AA',[None])]\n",
    "\n",
    "def parse_system(system):\n",
    "    \"\"\"\n",
    "    Parse a system string into a pair of projectiles and a beam energy.\n",
    "\n",
    "    \"\"\"\n",
    "    match = re.fullmatch('([A-Z]?[a-z])([A-Z]?[a-z])([0-9]+)', system)\n",
    "    return match.group(1, 2), int(match.group(3))\n",
    "\n",
    "\n",
    "class lazydict(dict):\n",
    "    \"\"\"\n",
    "    A dict that populates itself on demand by calling a unary function.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, function, *args, **kwargs):\n",
    "        self.function = function\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def __missing__(self, key):\n",
    "        self[key] = value = self.function(key, *self.args, **self.kwargs)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "design.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generates Latin-hypercube parameter designs.\n",
    "\n",
    "When run as a script, writes input files for use with my\n",
    "`heavy-ion collision event generator\n",
    "<https://github.com/jbernhard/heavy-ion-collisions-osg>`_.\n",
    "Run ``python -m src.design --help`` for usage information.\n",
    "\n",
    ".. warning::\n",
    "\n",
    "    This module uses the R `lhs package\n",
    "    <https://cran.r-project.org/package=lhs>`_ to generate maximin\n",
    "    Latin-hypercube samples.  As far as I know, there is no equivalent library\n",
    "    for Python (I am aware of `pyDOE <https://pythonhosted.org/pyDOE>`_, but\n",
    "    that uses a much more rudimentary algorithm for maximin sampling).\n",
    "\n",
    "    This means that R must be installed with the lhs package (run\n",
    "    ``install.packages('lhs')`` in an R session).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#from . import cachedir, parse_system, keys, labels, ranges, design_array, systems\n",
    "\n",
    "\n",
    "def generate_lhs(npoints, ndim, seed):\n",
    "    \"\"\"\n",
    "    Generate a maximin Latin-hypercube sample (LHS) with the given number of\n",
    "    points, dimensions, and random seed.\n",
    "\n",
    "    \"\"\"\n",
    "    logging.debug(\n",
    "        'generating maximin LHS: '\n",
    "        'npoints = %d, ndim = %d, seed = %d',\n",
    "        npoints, ndim, seed\n",
    "    )\n",
    "\n",
    "    cachefile = (\n",
    "        cachedir / 'lhs' /\n",
    "        'npoints{}_ndim{}_seed{}.npy'.format(npoints, ndim, seed)\n",
    "    )\n",
    "\n",
    "    if cachefile.exists():\n",
    "        logging.debug('loading from cache')\n",
    "        lhs = np.load(cachefile)\n",
    "    else:\n",
    "        logging.debug('not found in cache, generating using R')\n",
    "        proc = subprocess.run(\n",
    "            ['R', '--slave'],\n",
    "            input=\"\"\"\n",
    "            library('lhs')\n",
    "            set.seed({})\n",
    "            write.table(maximinLHS({}, {}), col.names=FALSE, row.names=FALSE)\n",
    "            \"\"\".format(seed, npoints, ndim).encode(),\n",
    "            stdout=subprocess.PIPE,\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "        lhs = np.array(\n",
    "            [l.split() for l in proc.stdout.splitlines()],\n",
    "            dtype=float\n",
    "        )\n",
    "\n",
    "        cachefile.parent.mkdir(exist_ok=True)\n",
    "        np.save(cachefile, lhs)\n",
    "\n",
    "    return lhs\n",
    "\n",
    "\n",
    "class Design:\n",
    "    \"\"\"\n",
    "    Latin-hypercube model design.\n",
    "\n",
    "    Creates a design for the given system with the given number of points.\n",
    "    Creates the main (training) design if `validation` is false (default);\n",
    "    creates the validation design if `validation` is true.  If `seed` is not\n",
    "    given, a default random seed is used (different defaults for the main and\n",
    "    validation designs).\n",
    "\n",
    "    Public attributes:\n",
    "\n",
    "    - ``system``: the system string\n",
    "    - ``projectiles``, ``beam_energy``: system projectile pair and beam energy\n",
    "    - ``type``: 'main' or 'validation'\n",
    "    - ``keys``: list of parameter keys\n",
    "    - ``labels``: list of parameter display labels (for TeX / matplotlib)\n",
    "    - ``range``: list of parameter (min, max) tuples\n",
    "    - ``min``, ``max``: numpy arrays of parameter min and max\n",
    "    - ``ndim``: number of parameters (i.e. dimensions)\n",
    "    - ``points``: list of design point names (formatted numbers)\n",
    "    - ``array``: the actual design array\n",
    "\n",
    "    The class also implicitly converts to a numpy array.\n",
    "\n",
    "    This is probably the worst class in this project, and certainly the least\n",
    "    generic.  It will probably need to be heavily edited for use in any other\n",
    "    project, if not completely rewritten.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, system, keys=keys, ranges=ranges,array = design_array, npoints=500, validation=False, seed=None):\n",
    "        self.system = system\n",
    "        self.projectiles, self.beam_energy = parse_system(system)\n",
    "        self.type = 'validation' if validation else 'main'\n",
    "        \n",
    "        self.keys = keys\n",
    "        self.range = ranges\n",
    "\n",
    "       # # 5.02 TeV has ~1.2x particle production as 2.76 TeV\n",
    "       # # [https://inspirehep.net/record/1410589]\n",
    "     #   norm_range = {\n",
    "     #       2760: (8., 20.),\n",
    "     #       5020: (10., 25.),\n",
    "     #   }[self.beam_energy]\n",
    "\n",
    "     #   self.keys, labels, self.range = map(list, zip(*[\n",
    "     #       ('norm',          r'{Norm}',                      (norm_range   )),\n",
    "     #       ('trento_p',      r'p',                           ( -0.5,    0.5)),\n",
    "     #       ('fluct_std',     r'\\sigma {fluct}',              (  0.0,    2.0)),\n",
    "     #       ('nucleon_width', r'w [{fm}]',                    (  0.4,    1.0)),\n",
    "     #       ('dmin3',         r'd {min} [{fm}]',              (  0.0, 1.7**3)),\n",
    "     #       ('tau_fs',        r'\\tau {fs} [{fm}/c]',          (  0.0,    1.5)),\n",
    "     #       ('etas_hrg',      r'\\eta/s {hrg}',                (  0.1,    0.5)),\n",
    "     #       ('etas_min',      r'\\eta/s {min}',                (  0.0,    0.2)),\n",
    "     #       ('etas_slope',    r'\\eta/s {slope} [{GeV}^{-1}]', (  0.0,    8.0)),\n",
    "     #       ('etas_crv',      r'\\eta/s {crv}',                ( -1.0,    1.0)),\n",
    "     #       ('zetas_max',     r'\\zeta/s {max}',               (  0.0,    0.1)),\n",
    "     #       ('zetas_width',   r'\\zeta/s {width} [{GeV}]',     (  0.0,    0.1)),\n",
    "     #       ('zetas_t0',      r'\\zeta/s T_0 [{GeV}]',         (0.150,  0.200)),\n",
    "     #       ('Tswitch',       r'T {switch} [{GeV}]',          (0.135,  0.165)),\n",
    "     #   ]))\n",
    "\n",
    "        # convert labels into TeX:\n",
    "        #   - wrap normal text with \\mathrm{}\n",
    "        #   - escape spaces\n",
    "        #   - surround with $$\n",
    "        self.labels = [\n",
    "            re.sub(r'({[A-Za-z]+})', r'\\mathrm\\1', i)\n",
    "            .replace(' ', r'\\ ')\n",
    "            .join('$$')\n",
    "            for i in labels\n",
    "        ]\n",
    "\n",
    "        self.ndim = len(self.range)\n",
    "        self.min, self.max = map(np.array, zip(*self.range))\n",
    "\n",
    "        # use padded numbers for design point names\n",
    "        fmt = '{:0' + str(len(str(npoints - 1))) + 'd}'\n",
    "        self.points = [fmt.format(i) for i in range(npoints)]\n",
    "\n",
    "\n",
    "        lhsmin = self.min.copy()\n",
    "\n",
    "        if seed is None:\n",
    "            seed = 751783496 if validation else 450829120\n",
    "\n",
    "        if array is None:\n",
    "            self.array = lhsmin + (self.max - lhsmin)*generate_lhs(\n",
    "                npoints=npoints, ndim=self.ndim, seed=seed\n",
    "            )\n",
    "        else:\n",
    "            self.array = array\n",
    "  \n",
    "\n",
    "    def __array__(self):\n",
    "        return self.array\n",
    "\n",
    "\n",
    "\n",
    "    def write_files(self, basedir):\n",
    "        \"\"\"\n",
    "        Write input files for each design point to `basedir`.\n",
    "\n",
    "        \"\"\"\n",
    "        outdir = basedir / self.type / self.system\n",
    "        outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for point, row in zip(self.points, self.array):\n",
    "            kwargs = dict(\n",
    "                zip(self.keys, row),\n",
    "                projectiles=self.projectiles,\n",
    "                cross_section={\n",
    "                    # sqrt(s) [GeV] : sigma_NN [fm^2]\n",
    "                    200: 4.2,\n",
    "                    2760: 6.4,\n",
    "                    5020: 7.0,\n",
    "                }[self.beam_energy]\n",
    "            )\n",
    "            kwargs.update(\n",
    "               fluct=1/kwargs.pop('fluct_std')**2,\n",
    "                dmin=kwargs.pop('dmin3')**(1/3),\n",
    "            )\n",
    "            filepath = outdir / point\n",
    "            with filepath.open('w') as f:\n",
    "                f.write(self._template.format(**kwargs))\n",
    "                logging.debug('wrote %s', filepath)\n",
    "\n",
    "    def print_array(self):\n",
    "        print('Design is')\n",
    "        print(self.array)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Making a Latin-Hypercube Design "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Make a random design 15 points with given ranges set seed to 150. Plot the second input versus the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = Design('PbPb5020',array=, npoints = ,seed=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x = ,y =)\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Make a design object with design_array, and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "design_obj = Design('PbPb5020',array=, keys=keys, labels=labels, ranges=ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x= , y = )\n",
    "plt.xlabel() #Hint - is there an attribute of design_obj that would make sense as an axis label?\n",
    "plt.ylabel()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "emulator.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains Gaussian process emulators.\n",
    "\n",
    "When run as a script, allows retraining emulators, specifying the number of\n",
    "principal components, and other options (however it is not necessary to do this\n",
    "explicitly --- the emulators will be trained automatically when needed).  Run\n",
    "``python -m src.emulator --help`` for usage information.\n",
    "\n",
    "Uses the `scikit-learn <http://scikit-learn.org>`_ implementations of\n",
    "`principal component analysis (PCA)\n",
    "<http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>`_\n",
    "and `Gaussian process regression\n",
    "<http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html>`_.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process import kernels\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from . import cachedir, lazydict, model, observables\n",
    "#from .design import Design\n",
    "\n",
    "\n",
    "class _Covariance:\n",
    "    \"\"\"\n",
    "    Proxy object to extract observable sub-blocks from a covariance array.\n",
    "    Returned by Emulator.predict().\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, array, slices):\n",
    "        self.array = array\n",
    "        self._slices = slices\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        (obs1, subobs1), (obs2, subobs2) = key\n",
    "        return self.array[\n",
    "            ...,\n",
    "            self._slices[obs1][subobs1],\n",
    "            self._slices[obs2][subobs2]\n",
    "        ]\n",
    "\n",
    "\n",
    "class Emulator:\n",
    "    \"\"\"\n",
    "    Multidimensional Gaussian process emulator using principal component\n",
    "    analysis.\n",
    "\n",
    "    The model training data are standardized (subtract mean and scale to unit\n",
    "    variance), then transformed through PCA.  The first `npc` principal\n",
    "    components (PCs) are emulated by independent Gaussian processes (GPs).  The\n",
    "    remaining components are neglected, which is equivalent to assuming they\n",
    "    are standard zero-mean unit-variance GPs.\n",
    "\n",
    "    This class has become a bit messy but it still does the job.  It would\n",
    "    probably be better to refactor some of the data transformations /\n",
    "    preprocessing into modular classes, to be used with an sklearn pipeline.\n",
    "    The classes would also need to handle transforming uncertainties, which\n",
    "    could be tricky.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    #Now takes a Design object as required input\n",
    "    def __init__(self, system, design, data_list, npc=10, nrestarts=0):\n",
    "        logging.info(\n",
    "            'training emulator for system %s (%d PC, %d restarts)',\n",
    "            system, npc, nrestarts,\n",
    "            'Design is',\n",
    "            design.array\n",
    "        )\n",
    "        \n",
    "        Y = []\n",
    "        self._slices = {}\n",
    "   \n",
    "        self.observables = observables\n",
    "    \n",
    "        # Build an array of all observables to emulate.\n",
    "        nobs = 0\n",
    "        for obs, subobslist in self.observables:\n",
    "            self._slices[obs] = {}\n",
    "            for subobs in subobslist:\n",
    "                Y.append(data_list[system][obs][subobs]['Y'])\n",
    "                n = Y[-1].shape[1]\n",
    "                self._slices[obs][subobs] = slice(nobs, nobs + n)\n",
    "                nobs += n\n",
    "\n",
    "        Y = np.concatenate(Y, axis=1)\n",
    "        pickle.dump(Y,open('mod_dat.p','wb'))\n",
    "\n",
    "        self.npc = npc\n",
    "        self.nobs = nobs\n",
    "        self.scaler = StandardScaler(copy=False)\n",
    "        self.pca = PCA(copy=False, whiten=True, svd_solver='full')\n",
    "\n",
    "        # Standardize observables and transform through PCA.  Use the first\n",
    "        # `npc` components but save the full PC transformation for later.\n",
    "        Z = self.pca.fit_transform(self.scaler.fit_transform(Y))[:, :npc]\n",
    "\n",
    "        self.Z = Z\n",
    "        # Define kernel (covariance function):\n",
    "        # Gaussian correlation (RBF) plus a noise term.\n",
    "        ptp = design.max - design.min\n",
    "        kernel = (\n",
    "            1. * kernels.RBF(\n",
    "                length_scale=ptp,\n",
    "                length_scale_bounds=np.outer(ptp, (.1, 10))\n",
    "            ) +\n",
    "            kernels.WhiteKernel(\n",
    "                noise_level=.1**2,\n",
    "                noise_level_bounds=(.01**2, 1)\n",
    "            )\n",
    "        )\n",
    "        # Fit a GP (optimize the kernel hyperparameters) to each PC.\n",
    "        self.gps = [\n",
    "            GPR(\n",
    "                kernel=kernel, alpha=0,\n",
    "                n_restarts_optimizer=nrestarts,\n",
    "                copy_X_train=False\n",
    "            ).fit(design, z)\n",
    "            for z in Z.T\n",
    "        ]\n",
    "\n",
    "        # Construct the full linear transformation matrix, which is just the PC\n",
    "        # matrix with the first axis multiplied by the explained standard\n",
    "        # deviation of each PC and the second axis multiplied by the\n",
    "        # standardization scale factor of each observable.\n",
    "        self._trans_matrix = (\n",
    "            self.pca.components_\n",
    "            * np.sqrt(self.pca.explained_variance_[:, np.newaxis])\n",
    "            * self.scaler.scale_\n",
    "        )\n",
    "\n",
    "        # Pre-calculate some arrays for inverse transforming the predictive\n",
    "        # variance (from PC space to physical space).\n",
    "\n",
    "        # Assuming the PCs are uncorrelated, the transformation is\n",
    "        #\n",
    "        #   cov_ij = sum_k A_ki var_k A_kj\n",
    "        #\n",
    "        # where A is the trans matrix and var_k is the variance of the kth PC.\n",
    "        # https://en.wikipedia.org/wiki/Propagation_of_uncertainty\n",
    "\n",
    "        # Compute the partial transformation for the first `npc` components\n",
    "        # that are actually emulated.\n",
    "        A = self._trans_matrix[:npc]\n",
    "        self._var_trans = np.einsum(\n",
    "            'ki,kj->kij', A, A, optimize=False).reshape(npc, nobs**2)\n",
    "\n",
    "        # Compute the covariance matrix for the remaining neglected PCs\n",
    "        # (truncation error).  These components always have variance == 1.\n",
    "        B = self._trans_matrix[npc:]\n",
    "        self._cov_trunc = np.dot(B.T, B)\n",
    "\n",
    "        # Add small term to diagonal for numerical stability.\n",
    "        self._cov_trunc.flat[::nobs + 1] += 1e-4 * self.scaler.var_\n",
    "\n",
    "    @classmethod\n",
    "    def from_cache(cls, system, retrain=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Load the emulator for `system` from the cache if available, otherwise\n",
    "        train and cache a new instance.\n",
    "\n",
    "        \"\"\"\n",
    "        cachefile = cachedir / 'emulator' / '{}.pkl'.format(system)\n",
    "\n",
    "        # cache the __dict__ rather than the Emulator instance itself\n",
    "        # this way the __name__ doesn't matter, e.g. a pickled\n",
    "        # __main__.Emulator can be unpickled as a src.emulator.Emulator\n",
    "        if not retrain and cachefile.exists():\n",
    "            logging.debug('loading emulator for system %s from cache', system)\n",
    "            emu = cls.__new__(cls)\n",
    "            emu.__dict__ = joblib.load(cachefile)\n",
    "            return emu\n",
    "\n",
    "        emu = cls(system, **kwargs)\n",
    "\n",
    "        logging.info('writing cache file %s', cachefile)\n",
    "        cachefile.parent.mkdir(exist_ok=True)\n",
    "        joblib.dump(emu.__dict__, cachefile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        return emu\n",
    "\n",
    "    def _inverse_transform(self, Z):\n",
    "        \"\"\"\n",
    "        Inverse transform principal components to observables.\n",
    "\n",
    "        Returns a nested dict of arrays.\n",
    "\n",
    "        \"\"\"\n",
    "        # Z shape (..., npc)\n",
    "        # Y shape (..., nobs)\n",
    "        Y = np.dot(Z, self._trans_matrix[:Z.shape[-1]])\n",
    "        Y += self.scaler.mean_\n",
    "\n",
    "        return {\n",
    "            obs: {\n",
    "                subobs: Y[..., s]\n",
    "                for subobs, s in slices.items()\n",
    "            } for obs, slices in self._slices.items()\n",
    "        }\n",
    "\n",
    "    def predict(self, X, return_cov=False, extra_std=0):\n",
    "        \"\"\"\n",
    "        Predict model output at `X`.\n",
    "\n",
    "        X must be a 2D array-like with shape ``(nsamples, ndim)``.  It is passed\n",
    "        directly to sklearn :meth:`GaussianProcessRegressor.predict`.\n",
    "\n",
    "        If `return_cov` is true, return a tuple ``(mean, cov)``, otherwise only\n",
    "        return the mean.\n",
    "\n",
    "        The mean is returned as a nested dict of observable arrays, each with\n",
    "        shape ``(nsamples, n_cent_bins)``.\n",
    "\n",
    "        The covariance is returned as a proxy object which extracts observable\n",
    "        sub-blocks using a dict-like interface:\n",
    "\n",
    "        >>> mean, cov = emulator.predict(X, return_cov=True)\n",
    "\n",
    "        >>> mean['dN_dy']['pion']\n",
    "        <mean prediction of pion dN/dy>\n",
    "\n",
    "        >>> cov[('dN_dy', 'pion'), ('dN_dy', 'pion')]\n",
    "        <covariance matrix of pion dN/dy>\n",
    "\n",
    "        >>> cov[('dN_dy', 'pion'), ('mean_pT', 'kaon')]\n",
    "        <covariance matrix between pion dN/dy and kaon mean pT>\n",
    "\n",
    "        The shape of the extracted covariance blocks are\n",
    "        ``(nsamples, n_cent_bins_1, n_cent_bins_2)``.\n",
    "\n",
    "        NB: the covariance is only computed between observables and centrality\n",
    "        bins, not between sample points.\n",
    "\n",
    "        `extra_std` is additional uncertainty which is added to each GP's\n",
    "        predictive uncertainty, e.g. to account for model systematic error.  It\n",
    "        may either be a scalar or an array-like of length nsamples.\n",
    "\n",
    "        \"\"\"\n",
    "        gp_mean = [gp.predict(X, return_cov=return_cov) for gp in self.gps]\n",
    "\n",
    "        if return_cov:\n",
    "            gp_mean, gp_cov = zip(*gp_mean)\n",
    "\n",
    "        mean = self._inverse_transform(\n",
    "            np.concatenate([m[:, np.newaxis] for m in gp_mean], axis=1)\n",
    "        )\n",
    "\n",
    "        if return_cov:\n",
    "            # Build array of the GP predictive variances at each sample point.\n",
    "            # shape: (nsamples, npc)\n",
    "            gp_var = np.concatenate([\n",
    "                c.diagonal()[:, np.newaxis] for c in gp_cov\n",
    "            ], axis=1)\n",
    "\n",
    "            # Add extra uncertainty to predictive variance.\n",
    "            extra_std = np.array(extra_std, copy=False).reshape(-1, 1)\n",
    "            gp_var += extra_std**2\n",
    "\n",
    "            # Compute the covariance at each sample point using the\n",
    "            # pre-calculated arrays (see constructor).\n",
    "            cov = np.dot(gp_var, self._var_trans).reshape(\n",
    "                X.shape[0], self.nobs, self.nobs\n",
    "            )\n",
    "            cov += self._cov_trunc\n",
    "\n",
    "            return mean, _Covariance(cov, self._slices)\n",
    "        else:\n",
    "            return mean\n",
    "\n",
    "    def sample_y(self, X, n_samples=1, random_state=None):\n",
    "        \"\"\"\n",
    "        Sample model output at `X`.\n",
    "\n",
    "        Returns a nested dict of observable arrays, each with shape\n",
    "        ``(n_samples_X, n_samples, n_cent_bins)``.\n",
    "\n",
    "        \"\"\"\n",
    "        # Sample the GP for each emulated PC.  The remaining components are\n",
    "        # assumed to have a standard normal distribution.\n",
    "        return self._inverse_transform(\n",
    "            np.concatenate([\n",
    "                gp.sample_y(\n",
    "                    X, n_samples=n_samples, random_state=random_state\n",
    "                )[:, :, np.newaxis]\n",
    "                for gp in self.gps\n",
    "            ] + [\n",
    "                np.random.standard_normal(\n",
    "                    (X.shape[0], n_samples, self.pca.n_components_ - self.npc)\n",
    "                )\n",
    "            ], axis=2)\n",
    "        )\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     #from . import systems\n",
    "#     def arg_to_system(arg):\n",
    "#         if arg not in systems:\n",
    "#             raise argparse.ArgumentTypeError(arg)\n",
    "#         return arg\n",
    "\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description='train emulators for each collision system',\n",
    "#         argument_default=argparse.SUPPRESS\n",
    "#     )\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         '--npc', type=int,\n",
    "#         help='number of principal components'\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--nrestarts', type=int,\n",
    "#         help='number of optimizer restarts'\n",
    "#     )\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         '--retrain', action='store_true',\n",
    "#         help='retrain even if emulator is cached'\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         'systems', nargs='*', type=arg_to_system,\n",
    "#         default=systems, metavar='SYSTEM',\n",
    "#         help='system(s) to train'\n",
    "#     )\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     kwargs = vars(args)\n",
    "\n",
    "#     for s in kwargs.pop('systems'):\n",
    "#         emu = Emulator.from_cache(s, **kwargs)\n",
    "\n",
    "#         print(s)\n",
    "#         print('{} PCs explain {:.5f} of variance'.format(\n",
    "#             emu.npc,\n",
    "#             emu.pca.explained_variance_ratio_[:emu.npc].sum()\n",
    "#         ))\n",
    "\n",
    "#         for n, (evr, gp) in enumerate(zip(\n",
    "#                 emu.pca.explained_variance_ratio_, emu.gps\n",
    "#         )):\n",
    "#             print(\n",
    "#                 'GP {}: {:.5f} of variance, LML = {:.5g}, kernel: {}'\n",
    "#                 .format(n, evr, gp.log_marginal_likelihood_value_, gp.kernel_)\n",
    "#             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Training GP Emulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Create an emulator object using design object, data_list_loaded, and 2 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emulators = Emulator(system='PbPb5020',design= ,data_list=, npc=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Make a plot of fraction of variance from the pca attribute of the emulator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hint - use previous exercises to help\n",
    "F_r = \n",
    "plt.plot(range(len(F_r)),F_r,'-o')\n",
    "plt.title('Fraction of Variance Explained')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('F_r')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Validation: Hold out design point \\#9, compare emulator predictions to truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holdout = \n",
    "train_rows = \n",
    "\n",
    "data_list_train = \n",
    "data_list_train['PbPb5020']['R_AA_2'][None]['Y'] = data_list_train['PbPb5020']['R_AA_2'][None]['Y'][train_rows]\n",
    "\n",
    "\n",
    "design_train = Design(array = ,keys=keys,ranges=ranges,system='PbPb5020')\n",
    "\n",
    "emulator_val = Emulator(system='PbPb5020',design = ,data_list = , npc=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note: This should be a (1,2) numpy array\n",
    "test_pt = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hint: use emulator_val\n",
    "pred_dict, pred_cov = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extracting the prediction mean and variance\n",
    "pred_pt = pred_dict['R_AA_2'][None][0]\n",
    "pred_var = np.diag(pred_cov.array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truth_pt = data_list_loaded['PbPb5020']['R_AA_2'][None]['Y'][holdout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plotting the truth against the predictions\n",
    "\n",
    "plt.scatter(x = truth_pt,y = )\n",
    "plt.errorbar(x = truth_pt, y = ,yerr = ,fmt='o')\n",
    "x = np.linspace(plt.axis()[0],plt.axis()[1],num = 1000)\n",
    "plt.plot(x,x,color = 'black')\n",
    "plt.title('Title')\n",
    "plt.ylabel('Emulator Predicted Value')\n",
    "plt.xlabel('True Hold-out Value')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mcmc.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Markov chain Monte Carlo model calibration using the `affine-invariant ensemble\n",
    "sampler (emcee) <http://dfm.io/emcee>`_.\n",
    "\n",
    "This module must be run explicitly to create the posterior distribution.\n",
    "Run ``python -m src.mcmc --help`` for complete usage information.\n",
    "\n",
    "On first run, the number of walkers and burn-in steps must be specified, e.g.\n",
    "::\n",
    "\n",
    "    python -m src.mcmc --nwalkers 500 --nburnsteps 100 200\n",
    "\n",
    "would run 500 walkers for 100 burn-in steps followed by 200 production steps.\n",
    "This will create the HDF5 file :file:`mcmc/chain.hdf` (default path).\n",
    "\n",
    "On subsequent runs, the chain resumes from the last point and the number of\n",
    "walkers is inferred from the chain, so only the number of production steps is\n",
    "required, e.g. ::\n",
    "\n",
    "    python -m src.mcmc 300\n",
    "\n",
    "would run an additional 300 production steps (total of 500).\n",
    "\n",
    "To restart the chain, delete (or rename) the chain HDF5 file.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "\n",
    "import emcee\n",
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.linalg import lapack\n",
    "from sklearn.externals import joblib\n",
    "#from . import workdir, systems, observables#, expt\n",
    "#from .design import Design\n",
    "#from .emulator import emulators\n",
    "import pickle\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def cov(\n",
    "        system, obs1, subobs1, obs2, subobs2,\n",
    "        stat_frac=1e-4, sys_corr_length=100, cross_factor=.8,\n",
    "        corr_obs={\n",
    "            frozenset({'dNch_deta', 'dET_deta', 'dN_dy'}),\n",
    "        }\n",
    "):\n",
    "    \"\"\"\n",
    "    Estimate a covariance matrix for the given system and pair of observables,\n",
    "    e.g.:\n",
    "\n",
    "    >>> cov('PbPb2760', 'dN_dy', 'pion', 'dN_dy', 'pion')\n",
    "    >>> cov('PbPb5020', 'dN_dy', 'pion', 'dNch_deta', None)\n",
    "\n",
    "    For each dataset, stat and sys errors are used if available.  If only\n",
    "    \"summed\" error is available, it is treated as sys error, and `stat_frac`\n",
    "    sets the fractional stat error.\n",
    "\n",
    "    Systematic errors are assumed to have a Gaussian correlation as a function\n",
    "    of centrality percentage, with correlation length set by `sys_corr_length`.\n",
    "\n",
    "    If obs{1,2} are the same but subobs{1,2} are different, the sys error\n",
    "    correlation is reduced by `cross_factor`.\n",
    "\n",
    "    If obs{1,2} are different and uncorrelated, the covariance is zero.  If\n",
    "    they are correlated, the sys error correlation is reduced by\n",
    "    `cross_factor`.  Two different obs are considered correlated if they are\n",
    "    both a member of one of the groups in `corr_obs` (the groups must be\n",
    "    set-like objects).  By default {Nch, ET, dN/dy} are considered correlated\n",
    "    since they are all related to particle / energy production.\n",
    "\n",
    "    \"\"\"\n",
    "    def unpack(obs, subobs):\n",
    "        dset = exp_data_list[system][obs][subobs]\n",
    "        yerr = dset['yerr']\n",
    "\n",
    "        try:\n",
    "            stat = yerr['stat']\n",
    "            sys = yerr['sys']\n",
    "        except KeyError:\n",
    "            stat = dset['y'] * stat_frac\n",
    "            sys = yerr['sum']\n",
    "\n",
    "        return dset['x'], stat, sys\n",
    "\n",
    "    x1, stat1, sys1 = unpack(obs1, subobs1)\n",
    "    x2, stat2, sys2 = unpack(obs2, subobs2)\n",
    "\n",
    "    if obs1 == obs2:\n",
    "        same_obs = (subobs1 == subobs2)\n",
    "    else:\n",
    "        # check if obs are both in a correlated group\n",
    "        if any({obs1, obs2} <= c for c in corr_obs):\n",
    "            same_obs = False\n",
    "        else:\n",
    "            return np.zeros((x1.size, x2.size))\n",
    "\n",
    "    # compute the sys error covariance\n",
    "    C = (\n",
    "        np.exp(-.5*(np.subtract.outer(x1, x2)/sys_corr_length)**2) *\n",
    "        np.outer(sys1, sys2)\n",
    "    )\n",
    "\n",
    "    if same_obs:\n",
    "        # add stat error to diagonal\n",
    "        C.flat[::C.shape[0]+1] += stat1**2\n",
    "    else:\n",
    "        # reduce correlation for different observables\n",
    "        C *= cross_factor\n",
    "\n",
    "    return C\n",
    "\n",
    "def mvn_loglike(y, cov):\n",
    "    \"\"\"\n",
    "    Evaluate the multivariate-normal log-likelihood for difference vector `y`\n",
    "    and covariance matrix `cov`:\n",
    "\n",
    "        log_p = -1/2*[(y^T).(C^-1).y + log(det(C))] + const.\n",
    "\n",
    "    The likelihood is NOT NORMALIZED, since this does not affect MCMC.  The\n",
    "    normalization const = -n/2*log(2*pi), where n is the dimensionality.\n",
    "\n",
    "    Arguments `y` and `cov` MUST be np.arrays with dtype == float64 and shapes\n",
    "    (n) and (n, n), respectively.  These requirements are NOT CHECKED.\n",
    "\n",
    "    The calculation follows algorithm 2.1 in Rasmussen and Williams (Gaussian\n",
    "    Processes for Machine Learning).\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the Cholesky decomposition of the covariance.\n",
    "    # Use bare LAPACK function to avoid scipy.linalg wrapper overhead.\n",
    "    L, info = lapack.dpotrf(cov, clean=False)\n",
    "\n",
    "    if info < 0:\n",
    "        raise ValueError(\n",
    "            'lapack dpotrf error: '\n",
    "            'the {}-th argument had an illegal value'.format(-info)\n",
    "        )\n",
    "    elif info < 0:\n",
    "        raise np.linalg.LinAlgError(\n",
    "            'lapack dpotrf error: '\n",
    "            'the leading minor of order {} is not positive definite'\n",
    "            .format(info)\n",
    "        )\n",
    "\n",
    "    # Solve for alpha = cov^-1.y using the Cholesky decomp.\n",
    "    alpha, info = lapack.dpotrs(L, y)\n",
    "\n",
    "    if info != 0:\n",
    "        raise ValueError(\n",
    "            'lapack dpotrs error: '\n",
    "            'the {}-th argument had an illegal value'.format(-info)\n",
    "        )\n",
    "\n",
    "    return -.5*np.dot(y, alpha) - np.log(L.diagonal()).sum()\n",
    "\n",
    "class LoggingEnsembleSampler(emcee.EnsembleSampler):\n",
    "    def run_mcmc(self, X0, nsteps, status=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Run MCMC with logging every 'status' steps (default: approx 10% of\n",
    "        nsteps).\n",
    "\n",
    "        \"\"\"\n",
    "        logging.info('running %d walkers for %d steps', self.k, nsteps)\n",
    "\n",
    "        if status is None:\n",
    "            status = nsteps // 10\n",
    "\n",
    "        for n, result in enumerate(\n",
    "                self.sample(X0, iterations=nsteps, **kwargs),\n",
    "                start=1\n",
    "        ):\n",
    "            if n % status == 0 or n == nsteps:\n",
    "                af = self.acceptance_fraction\n",
    "                logging.info(\n",
    "                    'step %d: acceptance fraction: '\n",
    "                    'mean %.4f, std %.4f, min %.4f, max %.4f',\n",
    "                    n, af.mean(), af.std(), af.min(), af.max()\n",
    "                )\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class Chain:\n",
    "    \"\"\"\n",
    "    High-level interface for running MCMC calibration and accessing results.\n",
    "\n",
    "    Currently all design parameters except for the normalizations are required\n",
    "    to be the same at all beam energies.  It is assumed (NOT checked) that all\n",
    "    system designs have the same parameters and ranges (except for the norms).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,emulators_dict, path=workdir / 'mcmc' / 'chain.hdf'):\n",
    "        self.path = path\n",
    "        self.path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "        \n",
    "        self.observables = observables\n",
    "        \n",
    "        self.emulators_dict = emulators_dict\n",
    "        # parameter order:\n",
    "        #  - normalizations (one for each system)\n",
    "        #  - all other physical parameters (same for all systems)\n",
    "        #  - model sys error\n",
    "        def keys_labels_range():\n",
    "            for sys in systems:\n",
    "                d = Design(sys)\n",
    "                klr = zip(d.keys, d.labels, d.range)\n",
    "                k, l, r = next(klr)\n",
    "                #assert k == 'lambda_jet'\n",
    "                yield (\n",
    "                    '{} {}'.format(k, sys),\n",
    "                    '{}\\n{:.2f} TeV'.format(l, d.beam_energy/1000),\n",
    "                    r\n",
    "                )\n",
    "\n",
    "            yield from klr\n",
    "\n",
    "            #yield 'model_sys_err', r'$\\sigma\\ \\mathrm{model\\ sys}$', (0., .4)\n",
    "        self.keys, self.labels, self.range = map(\n",
    "            list, zip(*keys_labels_range())\n",
    "        )\n",
    "\n",
    "        self.ndim = len(self.range)\n",
    "        self.min, self.max = map(np.array, zip(*self.range))\n",
    "\n",
    "        self._common_indices = list(range(len(systems), self.ndim))\n",
    "\n",
    "        self._slices = {}\n",
    "        self._expt_y = {}\n",
    "        self._expt_cov = {}\n",
    "\n",
    "        # pre-compute the experimental data vectors and covariance matrices\n",
    "        for sys, sysdata in exp_data_list.items():\n",
    "            nobs = 0\n",
    "\n",
    "            self._slices[sys] = []\n",
    "\n",
    "            for obs, subobslist in self.observables:\n",
    "                try:\n",
    "                    obsdata = sysdata[obs]\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "                for subobs in subobslist:\n",
    "                    try:\n",
    "                        dset = obsdata[subobs]\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "                    n = dset['y'].size\n",
    "                    self._slices[sys].append(\n",
    "                        (obs, subobs, slice(nobs, nobs + n))\n",
    "                    )\n",
    "                    nobs += n\n",
    "            self._expt_y[sys] = np.empty(nobs)\n",
    "            self._expt_cov[sys] = np.empty((nobs, nobs))\n",
    "\n",
    "            for obs1, subobs1, slc1 in self._slices[sys]:\n",
    "                self._expt_y[sys][slc1] = exp_data_list[sys][obs1][subobs1]['y']\n",
    "                for obs2, subobs2, slc2 in self._slices[sys]:\n",
    "                    self._expt_cov[sys][slc1, slc2] = cov(\n",
    "                        sys, obs1, subobs1, obs2, subobs2\n",
    "                    )\n",
    "            \n",
    "            if exp_cov is not None:\n",
    "                self._expt_cov[sys] = exp_cov\n",
    "    def _predict(self, X, **kwargs):\n",
    "        \"\"\"\n",
    "        Call each system emulator to predict model output at X.\n",
    "\n",
    "        \"\"\"\n",
    "        return {\n",
    "            sys: self.emulators_dict[sys].predict(\n",
    "                X[:, ],#[n] + self._common_indices],\n",
    "                **kwargs\n",
    "            )\n",
    "            for n, sys in enumerate(systems)\n",
    "        }\n",
    "\n",
    "    def log_posterior(self, X, extra_std_prior_scale=0):\n",
    "        \"\"\"\n",
    "        Evaluate the posterior at `X`.\n",
    "\n",
    "        `extra_std_prior_scale` is the scale parameter for the prior\n",
    "        distribution on the model sys error parameter:\n",
    "\n",
    "            prior ~ sigma^2 * exp(-sigma/scale)\n",
    "\n",
    "        \"\"\"\n",
    "        X = np.array(X, copy=False, ndmin=2)\n",
    "\n",
    "        lp = np.zeros(X.shape[0])\n",
    "\n",
    "        inside = np.all((X > self.min) & (X < self.max), axis=1)\n",
    "        lp[~inside] = -np.inf\n",
    "\n",
    "        nsamples = np.count_nonzero(inside)\n",
    "\n",
    "        if nsamples > 0:\n",
    "            #extra_std = X[inside, -1]\n",
    "            extra_std = 0.0\n",
    "            pred = self._predict(\n",
    "                X[inside], return_cov=True, extra_std=extra_std\n",
    "            )\n",
    "\n",
    "            for sys in systems:\n",
    "                nobs = self._expt_y[sys].size\n",
    "                # allocate difference (model - expt) and covariance arrays\n",
    "                dY = np.empty((nsamples, nobs))\n",
    "                cov = np.empty((nsamples, nobs, nobs))\n",
    "\n",
    "                model_Y, model_cov = pred[sys]\n",
    "\n",
    "                # copy predictive mean and covariance into allocated arrays\n",
    "                for obs1, subobs1, slc1 in self._slices[sys]:\n",
    "                    dY[:, slc1] = model_Y[obs1][subobs1]\n",
    "                    for obs2, subobs2, slc2 in self._slices[sys]:\n",
    "                        cov[:, slc1, slc2] = \\\n",
    "                            model_cov[(obs1, subobs1), (obs2, subobs2)]\n",
    "\n",
    "                # subtract expt data from model data\n",
    "                dY -= self._expt_y[sys]\n",
    "\n",
    "                # add expt cov to model cov\n",
    "                cov += self._expt_cov[sys]\n",
    "\n",
    "                # compute log likelihood at each point\n",
    "                lp[inside] += list(map(mvn_loglike, dY, cov))\n",
    "\n",
    "            # add prior for extra_std (model sys error)\n",
    "            #lp[inside] += 2*np.log(extra_std) - extra_std/extra_std_prior_scale\n",
    "\n",
    "        return lp\n",
    "\n",
    "    def random_pos(self, n=1):\n",
    "        \"\"\"\n",
    "        Generate `n` random positions in parameter space.\n",
    "\n",
    "        \"\"\"\n",
    "        return np.random.uniform(self.min, self.max, (n, self.ndim))\n",
    "\n",
    "    @staticmethod\n",
    "    def map(f, args):\n",
    "        \"\"\"\n",
    "        Dummy function so that this object can be used as a 'pool' for\n",
    "        :meth:`emcee.EnsembleSampler`.\n",
    "\n",
    "        \"\"\"\n",
    "        return f(args)\n",
    "\n",
    "    def run_mcmc(self, nsteps, nburnsteps=None, nwalkers=None, status=None):\n",
    "        \"\"\"\n",
    "        Run MCMC model calibration.  If the chain already exists, continue from\n",
    "        the last point, otherwise burn-in and start the chain.\n",
    "\n",
    "        \"\"\"\n",
    "        with self.open('a') as f:\n",
    "            try:\n",
    "                dset = f['chain']\n",
    "            except KeyError:\n",
    "                burn = True\n",
    "                if nburnsteps is None or nwalkers is None:\n",
    "                    logging.error(\n",
    "                        'must specify nburnsteps and nwalkers to start chain'\n",
    "                    )\n",
    "                    return\n",
    "                dset = f.create_dataset(\n",
    "                    'chain', dtype='f8',\n",
    "                    shape=(nwalkers, 0, self.ndim),\n",
    "                    chunks=(nwalkers, 1, self.ndim),\n",
    "                    maxshape=(nwalkers, None, self.ndim),\n",
    "                    compression='lzf'\n",
    "                )\n",
    "            else:\n",
    "                burn = False\n",
    "                nwalkers = dset.shape[0]\n",
    "\n",
    "            sampler = LoggingEnsembleSampler(\n",
    "                nwalkers, self.ndim, self.log_posterior, pool=self\n",
    "            )\n",
    "\n",
    "            if burn:\n",
    "                logging.info(\n",
    "                    'no existing chain found, starting initial burn-in')\n",
    "                # Run first half of burn-in starting from random positions.\n",
    "                nburn0 = nburnsteps // 2\n",
    "                sampler.run_mcmc(\n",
    "                    self.random_pos(nwalkers),\n",
    "                    nburn0,\n",
    "                    status=status\n",
    "                )\n",
    "                logging.info('resampling walker positions')\n",
    "                # Reposition walkers to the most likely points in the chain,\n",
    "                # then run the second half of burn-in.  This significantly\n",
    "                # accelerates burn-in and helps prevent stuck walkers.\n",
    "                X0 = sampler.flatchain[\n",
    "                    np.unique(\n",
    "                        sampler.flatlnprobability,\n",
    "                        return_index=True\n",
    "                    )[1][-nwalkers:]\n",
    "                ]\n",
    "                sampler.reset()\n",
    "                X0 = sampler.run_mcmc(\n",
    "                    X0,\n",
    "                    nburnsteps - nburn0,\n",
    "                    status=status,\n",
    "                    storechain=False\n",
    "                )[0]\n",
    "                sampler.reset()\n",
    "                logging.info('burn-in complete, starting production')\n",
    "            else:\n",
    "                logging.info('restarting from last point of existing chain')\n",
    "                X0 = dset[:, -1, :]\n",
    "\n",
    "            sampler.run_mcmc(X0, nsteps, status=status)\n",
    "\n",
    "            logging.info('writing chain to file')\n",
    "            dset.resize(dset.shape[1] + nsteps, 1)\n",
    "            dset[:, -nsteps:, :] = sampler.chain\n",
    "\n",
    "    def open(self, mode='r'):\n",
    "        \"\"\"\n",
    "        Return a handle to the chain HDF5 file.\n",
    "\n",
    "        \"\"\"\n",
    "        return h5py.File(str(self.path), mode)\n",
    "\n",
    "    @contextmanager\n",
    "    def dataset(self, mode='r', name='chain'):\n",
    "        \"\"\"\n",
    "        Context manager for quickly accessing a dataset in the chain HDF5 file.\n",
    "\n",
    "        >>> with Chain().dataset() as dset:\n",
    "                # do something with dset object\n",
    "\n",
    "        \"\"\"\n",
    "        with self.open(mode) as f:\n",
    "            yield f[name]\n",
    "\n",
    "    def load(self, *keys, thin=1):\n",
    "        \"\"\"\n",
    "        Read the chain from file.  If `keys` are given, read only those\n",
    "        parameters.  Read only every `thin`'th sample from the chain.\n",
    "\n",
    "        \"\"\"\n",
    "        if keys:\n",
    "            indices = [self.keys.index(k) for k in keys]\n",
    "            ndim = len(keys)\n",
    "            if ndim == 1:\n",
    "                indices = indices[0]\n",
    "        else:\n",
    "            ndim = self.ndim\n",
    "            indices = slice(None)\n",
    "\n",
    "        with self.dataset() as d:\n",
    "            return np.array(d[:, ::thin, indices]).reshape(-1, ndim)\n",
    "\n",
    "    def samples(self, n=1):\n",
    "        \"\"\"\n",
    "        Predict model output at `n` parameter points randomly drawn from the\n",
    "        chain.\n",
    "\n",
    "        \"\"\"\n",
    "        with self.dataset() as d:\n",
    "            X = np.array([\n",
    "                d[i] for i in zip(*[\n",
    "                    np.random.randint(s, size=n) for s in d.shape[:2]\n",
    "                ])\n",
    "            ])\n",
    "\n",
    "        return self._predict(X)\n",
    "\n",
    "\n",
    "def credible_interval(samples, ci=.9):\n",
    "    \"\"\"\n",
    "    Compute the highest-posterior density (HPD) credible interval (default 90%)\n",
    "    for an array of samples.\n",
    "\n",
    "    \"\"\"\n",
    "    # number of intervals to compute\n",
    "    nci = int((1 - ci)*samples.size)\n",
    "\n",
    "    # find highest posterior density (HPD) credible interval\n",
    "    # i.e. the one with minimum width\n",
    "    argp = np.argpartition(samples, [nci, samples.size - nci])\n",
    "    cil = np.sort(samples[argp[:nci]])   # interval lows\n",
    "    cih = np.sort(samples[argp[-nci:]])  # interval highs\n",
    "    ihpd = np.argmin(cih - cil)\n",
    "\n",
    "    return cil[ihpd], cih[ihpd]\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser(description='Markov chain Monte Carlo')\n",
    "\n",
    "#     parser.add_argument(\n",
    "#         'nsteps', type=int,\n",
    "#         help='number of steps'\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--nwalkers', type=int,\n",
    "#         help='number of walkers'\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--nburnsteps', type=int,\n",
    "#         help='number of burn-in steps'\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '--status', type=int,\n",
    "#         help='number of steps between logging status'\n",
    "#     )\n",
    "\n",
    "#     Chain().run_mcmc(**vars(parser.parse_args()))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Run the mcmc for 500 steps on 200 walkers, with 100 burn-in steps. Make a scatterplot of the draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emulators_dict_ex = {'PbPb5020':}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chain_ex = Chain(emulators_dict = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chain_ex.run_mcmc(nsteps = ,nburnsteps=,nwalkers = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hint: look for a function of a Chain object that will return what you need\n",
    "post_draws = \n",
    "plt.scatter(x = ,y = )\n",
    "plt.xlabel()\n",
    "plt.ylabel()\n",
    "plt.xlim((0,1))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting functions to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fontsmall, fontnormal, fontlarge = 5, 6, 7\n",
    "offblack = '#262626'\n",
    "aspect = 1/1.618\n",
    "resolution = 72.27\n",
    "textwidth = 307.28987/resolution\n",
    "textheight = 261.39864/resolution\n",
    "fullwidth = 350/resolution\n",
    "fullheight = 270/resolution\n",
    "\n",
    "from matplotlib import ticker\n",
    "from scipy.interpolate import PchipInterpolator\n",
    "\n",
    "from matplotlib import lines\n",
    "from matplotlib import patches\n",
    "\n",
    "\n",
    "def set_tight(fig=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Set tight_layout with a better default pad.\n",
    "    \"\"\"\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "\n",
    "    kwargs.setdefault('pad', .1)\n",
    "    fig.set_tight_layout(kwargs)\n",
    "\n",
    "def _posterior(chain,\n",
    "        params=None, ignore=None,\n",
    "        scale=1, padr=.99, padt=.98,\n",
    "        cmap=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Triangle plot of posterior marginal and joint distributions.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if params is None and ignore is None:\n",
    "        params = set(chain.keys)\n",
    "    elif params is not None:\n",
    "        params = set(params)\n",
    "    elif ignore is not None:\n",
    "        params = set(chain.keys) - set(ignore)\n",
    "\n",
    "    keys, labels, ranges = map(list, zip(*(\n",
    "        i for i in zip(chain.keys, chain.labels, chain.range)\n",
    "        if i[0] in params\n",
    "    )))\n",
    "    ndim = len(params)\n",
    "\n",
    "    data = chain.load(*keys).T\n",
    "\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "    cmap.set_bad('white')\n",
    "\n",
    "    line_color = cmap(.8)\n",
    "    fill_color = cmap(.5, alpha=.1)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=ndim, ncols=ndim,\n",
    "        sharex='col', sharey='row',\n",
    "        figsize=2*(scale*fullheight,)\n",
    "    )\n",
    "\n",
    "    for samples, key, lim, ax in zip(data, keys, ranges, axes.diagonal()):\n",
    "        counts, edges = np.histogram(samples, bins=50, range=lim)\n",
    "        x = (edges[1:] + edges[:-1]) / 2\n",
    "        y = .85 * (lim[1] - lim[0]) * counts / counts.max() + lim[0]\n",
    "        # smooth histogram with monotonic cubic interpolation\n",
    "        interp = PchipInterpolator(x, y)\n",
    "        x = np.linspace(x[0], x[-1], 10*x.size)\n",
    "        y = interp(x)\n",
    "        ax.plot(x, y, lw=.5, color=line_color)\n",
    "        ax.fill_between(x, lim[0], y, color=fill_color, zorder=-10)\n",
    "\n",
    "        ax.set_xlim(lim)\n",
    "        ax.set_ylim(lim)\n",
    "\n",
    "        if key == 'dmin3':\n",
    "            samples = samples**(1/3)\n",
    "\n",
    "#         ax.annotate(\n",
    "#             format_ci(samples), (.62, .92), xycoords='axes fraction',\n",
    "#             ha='center', va='bottom', fontsize=4.5\n",
    "#         )\n",
    "\n",
    "    for ny, nx in zip(*np.tril_indices_from(axes, k=-1)):\n",
    "        axes[ny][nx].hist2d(\n",
    "            data[nx], data[ny], bins=100,\n",
    "            range=(ranges[nx], ranges[ny]),\n",
    "            cmap=cmap, cmin=1\n",
    "        )\n",
    "        axes[nx][ny].set_axis_off()\n",
    "\n",
    "    for key, label, axb, axl in zip(keys, labels, axes[-1], axes[:, 0]):\n",
    "        for axis in [axb.xaxis, axl.yaxis]:\n",
    "            axis.set_label_text(label.replace(r'\\ [', '$\\n$['), fontsize=4)\n",
    "            axis.set_tick_params(labelsize=3)\n",
    "            if key == 'dmin3':\n",
    "                ticks = [0., 1.2, 1.5, 1.7]\n",
    "                axis.set_ticklabels(list(map(str, ticks)))\n",
    "                axis.set_ticks([t**3 for t in ticks])\n",
    "            else:\n",
    "                axis.set_major_locator(ticker.LinearLocator(3))\n",
    "                if (\n",
    "                        axis.axis_name == 'x'\n",
    "                        and scale / ndim < .13\n",
    "                        and any(len(str(x)) > 4 for x in axis.get_ticklocs())\n",
    "                ):\n",
    "                    for t in axis.get_ticklabels():\n",
    "                        t.set_rotation(30)\n",
    "\n",
    "        axb.get_xticklabels()[0].set_horizontalalignment('left')\n",
    "        axb.get_xticklabels()[-1].set_horizontalalignment('right')\n",
    "        axl.get_yticklabels()[0].set_verticalalignment('bottom')\n",
    "        axl.get_yticklabels()[-1].set_verticalalignment('top')\n",
    "\n",
    "    #set_tight(fig, pad=.05, h_pad=.1, w_pad=.1, rect=[0., 0., padr, padt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auto_ticks(ax, axis='both', minor=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Convenient interface to matplotlib.ticker locators.\n",
    "    \"\"\"\n",
    "    axis_list = []\n",
    "\n",
    "    if axis in {'x', 'both'}:\n",
    "        axis_list.append(ax.xaxis)\n",
    "    if axis in {'y', 'both'}:\n",
    "        axis_list.append(ax.yaxis)\n",
    "\n",
    "    for axis in axis_list:\n",
    "        axis.get_major_locator().set_params(**kwargs)\n",
    "        if minor:\n",
    "            axis.set_minor_locator(ticker.AutoMinorLocator(minor))\n",
    "\n",
    "def format_system(system):\n",
    "    \"\"\"\n",
    "    Format a system string into a display name, e.g.:\n",
    "    >>> format_system('PbPb2760')\n",
    "    'Pb+Pb 2.76 TeV'\n",
    "    >>> format_system('AuAu200')\n",
    "    'Au+Au 200 GeV'\n",
    "    \"\"\"\n",
    "    proj, energy = parse_system(system)\n",
    "\n",
    "    if energy > 1000:\n",
    "        energy /= 1000\n",
    "        prefix = 'T'\n",
    "    else:\n",
    "        prefix = 'G'\n",
    "\n",
    "    return '{} {} {}eV'.format('+'.join(proj), energy, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _observables(posterior=False,chain=None):\n",
    "    \"\"\"\n",
    "    Model observables at all design points or drawn from the posterior with\n",
    "    experimental data points.\n",
    "    \"\"\"\n",
    "    plots = _observables_plots()\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=len(plots), ncols=len(systems),\n",
    "        figsize=(.8*fullwidth, fullwidth),\n",
    "        gridspec_kw=dict(\n",
    "            height_ratios=[p.get('height_ratio', 1) for p in plots]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if posterior:\n",
    "        samples = chain.samples(100)\n",
    "\n",
    "\n",
    "    if(len(plots)==1&len(systems)==1):\n",
    "        axes=np.array(axes)\n",
    "\n",
    "\n",
    "    for (plot, system), ax in zip(\n",
    "            itertools.product(plots, systems), axes.flat\n",
    "    ):\n",
    "        for obs, subobs, opts in plot['subplots']:\n",
    "            #color = (10, 65, 55)\n",
    "            scale = opts.get('scale')\n",
    "\n",
    "            x = data_list_loaded[system][obs][subobs]['x']\n",
    "            Y = (\n",
    "                samples[system][obs][subobs]\n",
    "                if posterior else\n",
    "                data_list_loaded[system][obs][subobs]['Y']\n",
    "            )\n",
    "\n",
    "            if scale is not None:\n",
    "                Y = Y*scale\n",
    "\n",
    "            for y in Y:\n",
    "                ax.plot(x, y, color='red', alpha=.8, lw=.3)\n",
    "            if 'label' in opts:\n",
    "                ax.text(\n",
    "                    x[-1] + 3,\n",
    "                    np.median(Y[:, -1]),\n",
    "                    opts['label'],\n",
    "                    color=darken(color), ha='left', va='center'\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                dset = exp_data_list[system][obs][subobs]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            x = dset['x']\n",
    "            y = dset['y']\n",
    "            yerr = np.sqrt(sum(\n",
    "                e**2 for e in dset['yerr'].values()\n",
    "            ))\n",
    "\n",
    "            if scale is not None:\n",
    "                y = y*scale\n",
    "                yerr = yerr*scale\n",
    "\n",
    "            ax.errorbar(\n",
    "                x, y, yerr=yerr, fmt='o', ms=1.7,\n",
    "                capsize=0, color='.25', zorder=1000\n",
    "            )\n",
    "\n",
    "        if plot.get('yscale') == 'log':\n",
    "            ax.set_yscale('log')\n",
    "            ax.minorticks_off()\n",
    "        else:\n",
    "            auto_ticks(ax, 'y', nbins=4, minor=2)\n",
    "\n",
    "#        ax.set_xlim(0, 80)\n",
    "#        auto_ticks(ax, 'x', nbins=5, minor=2)\n",
    "\n",
    "#        ax.set_ylim(plot['ylim'])\n",
    "\n",
    "#         if ax.is_first_row():\n",
    "#             ax.set_title(format_system(system))\n",
    "#         elif ax.is_last_row():\n",
    "#             ax.set_xlabel('Centrality %')\n",
    "\n",
    "#         if ax.is_first_col():\n",
    "#             ax.set_ylabel(plot['ylabel'])\n",
    "\n",
    "#         if ax.is_last_col():\n",
    "#             ax.text(\n",
    "#                 1.02, .5, plot['title'],\n",
    "#                 transform=ax.transAxes, ha='left', va='center',\n",
    "#                 size=plt.rcParams['axes.labelsize'], rotation=-90\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _observables_plots():\n",
    "    \"\"\"\n",
    "    Metadata for observables plots.\n",
    "    \"\"\"\n",
    "    return[\n",
    "        dict(   title='Prior Draws',\n",
    "            ylabel=r'$R_{AA}$',\n",
    "            ylim=(0, .04),\n",
    "            subplots=[('R_AA_2', None, dict())]\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Make a pair plot of calibration draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_posterior(chain = , scale=1.6, padr=1., padt=.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Make plots of prior and posterior draws on top of experimental output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_observables(posterior = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_observables(posterior =  , chain = )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
